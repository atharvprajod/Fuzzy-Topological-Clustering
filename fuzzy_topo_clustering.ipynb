{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topological clustering\n",
      "----------------------\n",
      "Iteration: 0 -> Loss: 11.342406\n",
      "Iteration: 1 -> Loss: 11.155061\n",
      "Iteration: 2 -> Loss: 10.888361\n",
      "Iteration: 3 -> Loss: 10.740100\n",
      "Iteration: 4 -> Loss: 10.616048\n",
      "Iteration: 5 -> Loss: 10.575829\n",
      "\n",
      "Results\n",
      "-------\n",
      "True labels: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Pred indices: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Purity score: 0.9833333333333333\n",
      "Silhouette score: 0.01457604340894677\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "class TopClustering:\n",
    "    \"\"\"Topological clustering.\n",
    "    \n",
    "    Attributes:\n",
    "        n_clusters: \n",
    "          The number of clusters.\n",
    "        top_relative_weight:\n",
    "          Relative weight between the geometric and topological terms.\n",
    "          A floating point number between 0 and 1.\n",
    "        max_iter_alt:\n",
    "          Maximum number of iterations for the topological clustering.\n",
    "        max_iter_interp:\n",
    "          Maximum number of iterations for the topological interpolation.\n",
    "        learning_rate:\n",
    "          Learning rate for the topological interpolation.\n",
    "        \n",
    "    Reference:\n",
    "        Songdechakraiwut, Tananun, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, and Barry D. Van Veen. \n",
    "        \"Fast topological clustering with Wasserstein distance.\" \n",
    "        International Conference on Learning Representations (ICLR). 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter_alt,\n",
    "                 max_iter_interp, learning_rate):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.top_relative_weight = top_relative_weight\n",
    "        self.max_iter_alt = max_iter_alt\n",
    "        self.max_iter_interp = max_iter_interp\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"Computes topological clustering and predicts cluster index for each sample.\n",
    "        \n",
    "            Args:\n",
    "                data:\n",
    "                  Training instances to cluster.\n",
    "                  \n",
    "            Returns:\n",
    "                Cluster index each sample belongs to.\n",
    "        \"\"\"\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(\n",
    "            n_node - 2)  # n_edges = (n_node choose 2)\n",
    "        n_births = n_node - 1\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Networks represented as vectors concatenating geometric and topological info\n",
    "        X = []\n",
    "        for adj in data:\n",
    "            X.append(self._vectorize_geo_top_info(adj))\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Random initial condition\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "        # Assign the nearest centroid index to each data point\n",
    "        assigned_centroids = self._get_nearest_centroid(\n",
    "            X[:, None, :], self.centroids[None, :, :])\n",
    "        prev_assigned_centroids = assigned_centroids\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            for cluster in range(self.n_clusters):\n",
    "                # Previous iteration centroid\n",
    "                prev_centroid = np.zeros((n_node, n_node))\n",
    "                prev_centroid[np.triu_indices(\n",
    "                    prev_centroid.shape[0],\n",
    "                    k=1)] = self.centroids[cluster][:n_edges]\n",
    "\n",
    "                # Determine data points belonging to each cluster\n",
    "                cluster_members = X[assigned_centroids == cluster]\n",
    "\n",
    "                # Compute the sample mean and top. centroid of the cluster\n",
    "                cc = cluster_members.mean(axis=0)\n",
    "                sample_mean = np.zeros((n_node, n_node))\n",
    "                sample_mean[np.triu_indices(sample_mean.shape[0],\n",
    "                                            k=1)] = cc[:n_edges]\n",
    "                top_centroid = cc[n_edges:]\n",
    "                top_centroid_birth_set = top_centroid[:n_births]\n",
    "                top_centroid_death_set = top_centroid[n_births:]\n",
    "\n",
    "                # Update the centroid\n",
    "                try:\n",
    "                    cluster_centroid = self._top_interpolation(\n",
    "                        prev_centroid, sample_mean, top_centroid_birth_set,\n",
    "                        top_centroid_death_set)\n",
    "                    self.centroids[cluster] = self._vectorize_geo_top_info(\n",
    "                        cluster_centroid)\n",
    "                except:\n",
    "                    print(\n",
    "                        'Error: Possibly due to the learning rate is not within appropriate range.'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "\n",
    "            # Update the cluster membership\n",
    "            assigned_centroids = self._get_nearest_centroid(\n",
    "                X[:, None, :], self.centroids[None, :, :])\n",
    "\n",
    "            # Compute and print loss as it is progressively decreasing\n",
    "            loss = self._compute_top_dist(\n",
    "                X, self.centroids[assigned_centroids]).sum() / len(X)\n",
    "            print('Iteration: %d -> Loss: %f' % (it, loss))\n",
    "\n",
    "            if (prev_assigned_centroids == assigned_centroids).all():\n",
    "                break\n",
    "            else:\n",
    "                prev_assigned_centroids = assigned_centroids\n",
    "        return assigned_centroids\n",
    "\n",
    "    def _vectorize_geo_top_info(self, adj):\n",
    "        birth_set, death_set = self._compute_birth_death_sets(\n",
    "            adj)  # topological info\n",
    "        vec = adj[np.triu_indices(adj.shape[0], k=1)]  # geometric info\n",
    "        return np.concatenate((vec, birth_set, death_set), axis=0)\n",
    "\n",
    "    def _compute_birth_death_sets(self, adj):\n",
    "        \"\"\"Computes birth and death sets of a network.\"\"\"\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        return np.sort(mst[birth_ind]), np.sort(nonmst[death_ind])\n",
    "\n",
    "    def _bd_demomposition(self, adj):\n",
    "        \"\"\"Birth-death decomposition.\"\"\"\n",
    "        eps = np.nextafter(0, 1)\n",
    "        adj[adj == 0] = eps\n",
    "        adj = np.triu(adj, k=1)\n",
    "        Xcsr = csr_matrix(-adj)\n",
    "        Tcsr = minimum_spanning_tree(Xcsr)\n",
    "        mst = -Tcsr.toarray()  # reverse the negative sign\n",
    "        nonmst = adj - mst\n",
    "        return mst, nonmst\n",
    "\n",
    "    def _get_nearest_centroid(self, X, centroids):\n",
    "        \"\"\"Determines cluster membership of data points.\"\"\"\n",
    "        dist = self._compute_top_dist(X, centroids)\n",
    "        nearest_centroid_index = np.argmin(dist, axis=1)\n",
    "        return nearest_centroid_index\n",
    "\n",
    "    def _compute_top_dist(self, X, centroid):\n",
    "        \"\"\"Computes the pairwise top. distances between networks and centroids.\"\"\"\n",
    "        return np.dot((X - centroid)**2, self.weight_array)\n",
    "\n",
    "    def _top_interpolation(self, init_centroid, sample_mean,\n",
    "                           top_centroid_birth_set, top_centroid_death_set):\n",
    "        \"\"\"Topological interpolation.\"\"\"\n",
    "        curr = init_centroid\n",
    "        for _ in range(self.max_iter_interp):\n",
    "            # Geometric term gradient\n",
    "            geo_gradient = 2 * (curr - sample_mean)\n",
    "\n",
    "            # Topological term gradient\n",
    "            sorted_birth_ind, sorted_death_ind = self._compute_optimal_matching(\n",
    "                curr)\n",
    "            top_gradient = np.zeros_like(curr)\n",
    "            top_gradient[sorted_birth_ind] = top_centroid_birth_set\n",
    "            top_gradient[sorted_death_ind] = top_centroid_death_set\n",
    "            top_gradient = 2 * (curr - top_gradient)\n",
    "\n",
    "            # Gradient update\n",
    "            curr -= self.learning_rate * (\n",
    "                (1 - self.top_relative_weight) * geo_gradient +\n",
    "                self.top_relative_weight * top_gradient)\n",
    "        return curr\n",
    "\n",
    "    def _compute_optimal_matching(self, adj):\n",
    "        mst, nonmst = self._bd_demomposition(adj)\n",
    "        birth_ind = np.nonzero(mst)\n",
    "        death_ind = np.nonzero(nonmst)\n",
    "        sorted_temp_ind = np.argsort(mst[birth_ind])\n",
    "        sorted_birth_ind = tuple(np.array(birth_ind)[:, sorted_temp_ind])\n",
    "        sorted_temp_ind = np.argsort(nonmst[death_ind])\n",
    "        sorted_death_ind = tuple(np.array(death_ind)[:, sorted_temp_ind])\n",
    "        return sorted_birth_ind, sorted_death_ind\n",
    "\n",
    "\n",
    "#############################################\n",
    "################### Demo ####################\n",
    "#############################################\n",
    "def random_modular_graph(d, c, p, mu, sigma):\n",
    "    \"\"\"Simulated modular network.\n",
    "    \n",
    "        Args:\n",
    "            d: Number of nodes.\n",
    "            c: Number of clusters/modules.\n",
    "            p: Probability of attachment within module.\n",
    "            mu, sigma: Used for random edge weights.\n",
    "            \n",
    "        Returns:\n",
    "            Adjacency matrix.\n",
    "    \"\"\"\n",
    "    adj = np.zeros((d, d))  # adjacency matrix\n",
    "    for i in range(1, d + 1):\n",
    "        for j in range(i + 1, d + 1):\n",
    "            module_i = math.ceil(c * i / d)\n",
    "            module_j = math.ceil(c * j / d)\n",
    "\n",
    "            # Within module\n",
    "            if module_i == module_j:\n",
    "                if random.uniform(0, 1) <= p:\n",
    "                    w = np.random.normal(mu, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "                else:\n",
    "                    w = np.random.normal(0, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "\n",
    "            # Between modules\n",
    "            else:\n",
    "                if random.uniform(0, 1) <= 1 - p:\n",
    "                    w = np.random.normal(mu, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "                else:\n",
    "                    w = np.random.normal(0, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def purity_score(labels_true, labels_pred):\n",
    "    mtx = contingency_matrix(labels_true, labels_pred)\n",
    "    return np.sum(np.amax(mtx, axis=0)) / np.sum(mtx)\n",
    "\n",
    "\n",
    "def main():\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # Generate a dataset comprising simulated modular networks\n",
    "    dataset = []\n",
    "    labels_true = []\n",
    "    n_network = 20\n",
    "    n_node = 60\n",
    "    p = 0.7\n",
    "    mu = 1\n",
    "    sigma = 0.5\n",
    "    for module in [2, 3, 5]:\n",
    "        for _ in range(n_network):\n",
    "            adj = random_modular_graph(n_node, module, p, mu, sigma)\n",
    "            # Uncomment lines below for visualization\n",
    "            # plt.imshow(adj, vmin=0, vmax=2, cmap='YlOrRd')\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "            dataset.append(adj)\n",
    "            labels_true.append(module)\n",
    "\n",
    "    # Topological clustering\n",
    "    n_clusters = 3\n",
    "    top_relative_weight = 0.99  # 'top_relative_weight' between 0 and 1\n",
    "    max_iter_alt = 300\n",
    "    max_iter_interp = 300\n",
    "    learning_rate = 0.05\n",
    "    print('Topological clustering\\n----------------------')\n",
    "    labels_pred_kmeans = TopClustering(n_clusters, top_relative_weight, max_iter_alt,\n",
    "                                max_iter_interp,\n",
    "                                learning_rate).fit_predict(dataset)\n",
    "    print('\\nResults\\n-------')\n",
    "    print('True labels:', np.asarray(labels_true))\n",
    "    print('Pred indices:', labels_pred_kmeans)\n",
    "    print('Purity score:', purity_score(labels_true, labels_pred_kmeans))\n",
    "    flattened_dataset = [x.flatten() for x in dataset]\n",
    "    print('Silhouette score:', silhouette_score(flattened_dataset, labels_pred_kmeans))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyTopClustering(TopClustering):\n",
    "    def __init__(self, n_clusters, top_relative_weight, max_iter, m):\n",
    "        super().__init__(n_clusters, top_relative_weight, max_iter, max_iter, 0.05)\n",
    "        self.m = m\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        data = np.asarray(data)\n",
    "        n_node = data.shape[1]\n",
    "        n_edges = math.factorial(n_node) // math.factorial(2) // math.factorial(n_node - 2)\n",
    "        self.weight_array = np.append(\n",
    "            np.repeat(1 - self.top_relative_weight, n_edges),\n",
    "            np.repeat(self.top_relative_weight, n_edges))\n",
    "\n",
    "        # Continue as before\n",
    "        X = [self._vectorize_geo_top_info(adj) for adj in data]\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Initialize fuzzy membership matrix (randomly)\n",
    "        self.U = np.random.rand(X.shape[0], self.n_clusters)\n",
    "        self.U = self.U / np.tile(self.U.sum(axis=1)[np.newaxis].T, self.n_clusters)\n",
    "\n",
    "        # Initialize centroids\n",
    "        self.centroids = X[random.sample(range(X.shape[0]), self.n_clusters)]\n",
    "\n",
    "\n",
    "        for it in range(self.max_iter_alt):\n",
    "            # Update centroids\n",
    "            for j in range(self.n_clusters):\n",
    "                num = np.dot((self.U[:, j] ** self.m), X)\n",
    "                den = np.sum(self.U[:, j] ** self.m)\n",
    "                self.centroids[j] = num / den\n",
    "\n",
    "            # Update membership matrix U\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(self.n_clusters):\n",
    "                    top_distance = self._compute_top_dist(X[i], self.centroids[j])\n",
    "                    sum_term = sum([(top_distance / self._compute_top_dist(X[i], self.centroids[k])) ** (2 / (self.m - 1)) for k in range(self.n_clusters)])\n",
    "                    self.U[i, j] = 1 / sum_term\n",
    "\n",
    "        # Return the cluster with maximum membership as the hard clustering result\n",
    "        return np.argmax(self.U, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Topological clustering\n",
      "----------------------\n",
      "\n",
      "Results\n",
      "-------\n",
      "True labels: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Pred indices: [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Purity score: 0.9833333333333333\n",
      "Silhouette score: 0.015274093550167546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#############################################\n",
    "################### Demo ####################\n",
    "#############################################\n",
    "def random_modular_graph(d, c, p, mu, sigma):\n",
    "    \"\"\"Simulated modular network.\n",
    "    \n",
    "        Args:\n",
    "            d: Number of nodes.\n",
    "            c: Number of clusters/modules.\n",
    "            p: Probability of attachment within module.\n",
    "            mu, sigma: Used for random edge weights.\n",
    "            \n",
    "        Returns:\n",
    "            Adjacency matrix.\n",
    "    \"\"\"\n",
    "    adj = np.zeros((d, d))  # adjacency matrix\n",
    "    for i in range(1, d + 1):\n",
    "        for j in range(i + 1, d + 1):\n",
    "            module_i = math.ceil(c * i / d)\n",
    "            module_j = math.ceil(c * j / d)\n",
    "\n",
    "            # Within module\n",
    "            if module_i == module_j:\n",
    "                if random.uniform(0, 1) <= p:\n",
    "                    w = np.random.normal(mu, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "                else:\n",
    "                    w = np.random.normal(0, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "\n",
    "            # Between modules\n",
    "            else:\n",
    "                if random.uniform(0, 1) <= 1 - p:\n",
    "                    w = np.random.normal(mu, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "                else:\n",
    "                    w = np.random.normal(0, sigma)\n",
    "                    adj[i - 1][j - 1] = max(w, 0)\n",
    "    return adj\n",
    "def purity_score(labels_true, labels_pred):\n",
    "    mtx = contingency_matrix(labels_true, labels_pred)\n",
    "    return np.sum(np.amax(mtx, axis=0)) / np.sum(mtx)\n",
    "\n",
    "def main():\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # Generate a dataset comprising simulated modular networks\n",
    "    dataset = []\n",
    "    labels_true = []\n",
    "    n_network = 20\n",
    "    n_node = 60\n",
    "    p = 0.7\n",
    "    mu = 1\n",
    "    sigma = 0.5\n",
    "    for module in [2, 3, 5]:\n",
    "        for _ in range(n_network):\n",
    "            adj = random_modular_graph(n_node, module, p, mu, sigma)\n",
    "            # Uncomment lines below for visualization\n",
    "            # plt.imshow(adj, vmin=0, vmax=2, cmap='YlOrRd')\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "            dataset.append(adj)\n",
    "            labels_true.append(module)\n",
    "\n",
    "    # Fuzzy topological clustering\n",
    "    n_clusters = 3\n",
    "    top_relative_weight = 0.99  # 'top_relative_weight' between 0 and 1\n",
    "    max_iter = 300\n",
    "    m = 2  # fuzziness parameter\n",
    "    print('Fuzzy Topological clustering\\n----------------------')\n",
    "    labels_pred_fuzzy = FuzzyTopClustering(n_clusters, top_relative_weight, max_iter, m).fit_predict(dataset)\n",
    "    \n",
    "    print('\\nResults\\n-------')\n",
    "    print('True labels:', np.asarray(labels_true))\n",
    "    print('Pred indices:', labels_pred_fuzzy)\n",
    "    print('Purity score:', purity_score(labels_true, labels_pred_fuzzy))\n",
    "    flattened_dataset = [x.flatten() for x in dataset]\n",
    "    print('Silhouette score:', silhouette_score(flattened_dataset, labels_pred_fuzzy))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
